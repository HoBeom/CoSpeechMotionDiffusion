# 연구노트 

본 연구노트는 release 후 제거될 예정입니다. 
김형민 학생의 연구노트를 참고하였습니다.

## 연구 목표

Co-speech motion generation with diffusion model

## 계획 및 진행 상황 

1주차 (03.20 ~ 03.24): 

- [X] Motion 생성 모델 관련 자료조사

- [x] AI-Hub 데이터셋 다운로드 및 audio, skeleton, text 파일 추출 

- [ ] TED 데이터셋에 대해 yoon et al. 코드 돌려보고 구조 확인/검토

2주차 (03.27 ~ 03.31) : 

- [ ] MLD: Motion latent diffusion 논문 리뷰

- [ ] MDM: Motion Diffusion Model 논문 리뷰

- [ ] MLD 코드 돌려보고 구조 확인 및 검토

3주차 (04.03 ~ 04.07) : 

- [ ] AI-Hub 데이터 로더 구현

- [ ] Audio Condition idea 논의

4주차 (04.10 ~ 04.14) : 

5주차 (05.01 ~ 05.05) : 

6주차 (05.08 ~ 05.12) : 

7주차 (05.15 ~ 05.19) : 


8주차 이후 (05.22 ~ ) : 



## 연구 설계 

### 1. baseline 확보 : AI-Hub 

먼저 AI-Hub의 베이스라인을 돌려 보고 데이터 전처리도 수행한다. 

### 2. 이전 연구 공부 

타 방법들을 공부하고 구현 해 본다. 

#### Reviews 

- [X] [Generative models for human’s motion](/note/MotionGeneration.pdf)

#### References

1. [Youtube Gesture Dataset](https://github.com/youngwoo-yoon/youtube-gesture-dataset)

2. [Gesture Generation from trimodal context](https://github.com/ai4r/Gesture-Generation-from-Trimodal-Context)

3. [Motion Latent Diffusion](https://github.com/ChenFengYe/motion-latent-diffusion)

4. [MDM: Human Motion Diffusion Model](https://github.com/GuyTevet/motion-diffusion-model)


## 3. 아이디어 노트 

## 4. 실험 

## 5. 잠정적인 결론들 

## 6. 논문 작성 


